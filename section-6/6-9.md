# Installing MongoDB

We need to install mongodb in our virtual machine as well as the connector

So we need to do the following commands with ```su root```

```
cd /var/lib/ambari-server/resources/stacks
cd HDP
cd 2.6
cd services
pwd

git clone https://github.com/nikunjness/mongo-ambari.git

sudo service ambari restart
```

Now we can log into ambari through the browser.

We go to actions -> add service -> mongoDB and TICK

And accept defaults **and deploy!**

MongoDB can be quite picky. If you shut it down incorrectly it might not start up again....

If you get into that state, you might have to delete the image and reinstall the entire image and start from scratch... this time will hopefully start successfully hopefully...

Finally let's just click complete.

Now we can start playing in Ambari...

We can do the same thing we did in cassandra of copying all the movielens data.

Let's check that the data is all in the files view.

Now we need a python spark script to integrate it.

So now we go back to session and type:

```
pip install pymongo=3.4.0
```

In regards to our script we have:

``` python
from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql import functions

def parseInput(line):
    fields = line.split('|')
    return Row(user_id = int(fields[0]), age = int(fields[1]), gender = fields[2], occupation = fields[3], zip = fields[4])

if __name__ == "__main__":
    # Create a SparkSession
    spark = SparkSession.builder.appName("MongoDBIntegration").getOrCreate()

    # Get the raw data
    lines = spark.sparkContext.textFile("hdfs:///user/maria_dev/ml-100k/u.user")
    # Convert it to a RDD of Row objects with (userID, age, gender, occupation, zip)
    users = lines.map(parseInput)
    # Convert that to a DataFrame
    usersDataset = spark.createDataFrame(users)

    # Write it into MongoDB
    usersDataset.write\
        .format("com.mongodb.spark.sql.DefaultSource")\
        .option("uri","mongodb://127.0.0.1/movielens.users")\
        .mode('append')\
        .save()

    # Read it back from MongoDB into a new Dataframe
    readUsers = spark.read\
    .format("com.mongodb.spark.sql.DefaultSource")\
    .option("uri","mongodb://127.0.0.1/movielens.users")\
    .load()

    readUsers.createOrReplaceTempView("users")

    sqlDF = spark.sql("SELECT * FROM users WHERE age < 20")
    sqlDF.show()

    # Stop the session
    spark.stop()
```

* Get the data, and convert into dataframe
* The code is almost exactly to the one for cassandra
    - Only difference is using a different driver/connector
* We specify where the mongo server is later
* We parse function to create a dataframe of rows
    - Rows contain user_id, age, geneder, occupation and zip
* Create a dataframe
* And write out!
    - userdataset.write
        + .format("defaultsource").option("uri", "mongodb/...")
        + .mode("append")
        + .save
* Now we read it back in 
    - readusers = spark.read
        + format("com......defaultsource")
        + .option("uri", "mongodb://...")
        + .load()
* Now we can just run an sql query
    - spark.sql("SELECT * FROM users WHERE age < 20")

We are not loading the data, but we are figuring out how to run query in the mongodb data in the database itself!

**Now let's run it**

Back into session, let's run:

```
cd ~
wget http://media.sundog-soft.com/hadoop/MongoSpark.py

# We need to tell to use SPARK 2
export SPARK_MAJOR_VERSION=2

spark-submit --packages org.mongodb.spark:mongo-spark-connector_2.11:2.0.0 MongoSpark.py
```

And now we can see the results of the read-back of the information we stored

We also can see that _id got written automatically!

