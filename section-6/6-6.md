
# Installing Cassandra

It will not be as out of the box simple. 

We start like always by ssh-ing with maria_dev, and opening root.

First thing we need to do is install Python 2.7. We can't just upgrade python as CentOS needs python 2.6...

We start by updating yum and install scl-utils to move around python, and centos-release-scl-rh, and finally python27

```
yum update
yum install scl-utils
yum install centos-release-scl-rh
yum install python27

```

Now switch to python 2.7 with ```scl enable python27 bash```. And now we've done it in a way that doesn't break the OS.

Now we can install cassandra itself. Unfortunatelly we can't just install yum install cassandra, so we'll do the following

```
cd /etc/yum.repos.d
```

And we create a file datastax.repo with the following

```
[datastax]
name = DataStax Repo for Apache Cassandra
baseurl = http://rpm.datastax.com/community
enabled = 1
gpgcheck = 0
```
[datastax]: This is a section header enclosed in square brackets. It specifies the name of the repository.

name = DataStax Repo for Apache Cassandra: This line defines the human-readable name for the repository, which is "DataStax Repo for Apache Cassandra" in this case.

baseurl = http://rpm.datastax.com/community: This line specifies the base URL from which the package manager (e.g., YUM) can download packages. In this case, it points to the DataStax repository for Apache Cassandra.

enabled = 1: This line indicates that the repository is enabled. When set to "1," it means the repository is active and packages can be installed or updated from it.

gpgcheck = 0: This line sets the "gpgcheck" option to "0," which means that package signature checking is disabled. In other words, packages from this repository will not be checked for cryptographic signatures.

Please note that setting "gpgcheck" to "0" means you are not verifying the authenticity of packages from this repository, which may pose a security risk. It's important to ensure that you trust the source of the packages before using such a repository in a production environment.

This configuration file is typically placed in a repository configuration directory on your Linux system (e.g., /etc/yum.repos.d/ on Red Hat-based distributions) to enable package management tools like YUM to access and install software from the specified repository.

NOW we can install it with ```yum install dsc30```.

Done!

Well now we install dependencies:

```
pip install cqlsh
```

Finally we can run cassandra:

```
service cassandra start
```


Now cassandra is running, let's setup a table.

We can't just run cqlsh, we need to make sure it's talking to the right version.

```
cqlsh --cqlversion="3.4.0"
```

Finally we're in!

Now we can actually create a table... for the movielens users

```
CREATE KEYSPACE movielens WITH replication = {'class': 'SimpleStrategy', 'replication_factor':'1'} AND durable_writes = true;
```
CREATE KEYSPACE movielens: This part of the statement initiates the creation of a keyspace named "movielens."

WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}:The WITH replication clause specifies the replication strategy and its options for the keyspace.

{'class': 'SimpleStrategy'} indicates that you are using the "SimpleStrategy" replication strategy. SimpleStrategy is suitable for single-data-center deployments where you specify a replication factor (as follows).

'replication_factor': '1' sets the replication factor to 1, meaning that there is only one copy of the data. In a production environment, you would typically set a higher replication factor for data redundancy and fault tolerance across multiple nodes.

AND durable_writes = true;:The durable_writes option is set to "true," which means that writes to this keyspace will be persisted to disk and are considered durable.
This should create our keyspace (ie database)

Now let's use it.

```
use movielens;
CREATE TABLE users (user_id int, age int, gender text, occupation text, zip text, PRIMARY KEY (user_id)); 
DESCRIBE users;

# Let's check it's empty

SELECT * FROM users;
```

Now we'll use a spark script to run it!




